# ğŸ§  KV Cache and Paged Attention Understanding

## ğŸ“š Background
You have a single **NVIDIA H100 GPU** with **80 GB of VRAM**.  
You are running an inference engine that uses the **traditional KV-cache mechanism** (i.e., **no Paged Attention**).  

You load a Causal LM called **MyLLM** with the following architecture:

- ğŸ—ï¸ **Model architecture:** 3 Transformer blocks (Attention + MLP in each block)  
- ğŸ¯ **Multi-head attention:** 8 heads (standard, no GQA, no MLA)  
- ğŸ“ **Model dimension:** 2048  
- ğŸ”¹ **Per-head dimension:** 2048 Ã· 8 = 256  
- ğŸ“ **Q, K, V projection weight shapes:** 256 Ã— 2048   
- ğŸ“ **Context length per request:** 100,000 tokens  

---

## âš¡ Assumptions
1. The model itself occupies **20 GB VRAM**.  
2. All other memory (activations, temporary buffers, etc.) is negligible â€” only the KV-cache memory limits parallelism.  
3. K/V tensors are stored in **float16 (2 bytes per value)**.

---

## â“ Question
For an inference engine **without Paged Attention**:

1. ğŸ§® Compute the **KV cache size per request** based on the model description.  
2. ğŸš€ Compute the **maximum number of requests** that can be served in parallel on this GPU before running out of memory.
