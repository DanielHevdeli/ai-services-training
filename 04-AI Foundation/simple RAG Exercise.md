# ğŸ§ª RAG from Scratch â€” Handling Data the Model Was Never Trained On

## ğŸ¯ Objective

This exercise builds an intuitive, hands-on understanding of **Retrieval-Augmented Generation (RAG)**.

By the end of this exercise, you should understand:
- Why pretrained LLMs fail on **new, private, or changing data**
- How retrieval grounds the model in external knowledge without fine-tune it
- Why embeddings and vector databases exist
- How retrieved context changes model behavior
- Why choosing the right `top_k` value matters

You will use our LLMs and Embedders. Ask your trainee how to use them.

---

## Step 1 â€” Ask a Question the Model Cannot Know ğŸš«ğŸ“š

Ask one of our LLMs the following question:
""

### Think About ğŸ¤”
- Is the model actually wrong, or just unaware?
- Why canâ€™t a pretrained model access this information?
- Would retraining the model every time data changes be realistic?

ğŸ’¡ Key Insight  
A pretrained LLM is **frozen in time** and has no access to your private or updated data.

---

## Step 2 â€” Introduce External Knowledge ğŸ—‚ï¸

Now use a list of updated facts from 2025_facts.py
As you noticed, this data lives **outside** the model and hence must be retrieved at query time.

---

## Step 3 â€” Build a Simplified â€œVector Databaseâ€ ğŸ§±

Use one of our Embedders to create a simple python dictionary to funciton as our "vector database" 

âš ï¸ **IMPORTANT NOTE**  
--------------------------------------------------
You are NOT building a real vector database here.

In production systems, we use **vector databases**
(e.g. FAISS, Pinecone, Weaviate).

Why are vector databases needed?
- ğŸ“Š Embeddings are large numeric vectors
- ğŸ“š Systems often store millions of documents
- âš¡ We need extremely fast nearest-neighbor search
- âŒ Relational databases cannot efficiently answer:
  â€œWhich documents are semantically closest to this query?â€

Your in-memory structure exists ONLY to understand the RAG flow.

---

## Step 4 â€” Implement a RAG Pipeline ğŸšï¸ğŸ”

Choose one of our LLMs and write a python script that gets a question from the user and answer using your vector store

Use a `top_k` of your choise

---

## Step 9 â€” Experiment with top_k ğŸ§ª

Try different values for `top_k`.

Reflect:
- Does `top_k = 1` always succeed? Try it.
- When does increasing `top_k` improve answers?
- When does it start harming answer quality?
- How does irrelevant context confuse the model?
- Summarize the main Trade-offs of low vs high `top_k` (Fast, Cheap, Missing important knowledge, Noise)
  - âŒ Risk of missing important context
    
---

## âœ… Key Takeaway

âœ¨ **RAG does not make an LLM smarter.  
It makes the LLM better informed.**

The model stays the same â€” only the **context and instructions** change.
