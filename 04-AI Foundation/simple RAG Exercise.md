# ğŸ§ª RAG from Scratch â€” Working with Data the Model Has Never Seen

## ğŸ¯ Goal

This exercise gives you hands-on experience with **Retrieval-Augmented Generation (RAG)**.

By the end, youâ€™ll understand:  
- Why pretrained LLMs struggle with **new, private, or evolving data**  
- How retrieval lets models access external knowledge **without retraining**  
- The role of **embeddings and vector databases**  
- How retrieved context affects model responses

Youâ€™ll use our LLMs and Embedders â€” ask your trainee how to interact with them.  

---

## Step 1 â€” Ask an â€œUnknowableâ€ Question ğŸš«ğŸ“š

Ask one of our LLMs a question about thing that happened in 2025. For instance:
"Which country was the first to recognize Somaliland, and under which Prime Minister or President did it happen?"
Observe it's answer.

### Reflection ğŸ¤”
- Is the model actually wrong, or just **uninformed**?
- Why canâ€™t a pretrained model access this type of data?
- Would retraining the model every time the data updates be realistic?

ğŸ’¡ Insight:  
A pretrained LLM is **frozen in time** and cannot access private or updated information.

---

## Step 2 â€” Bring in External Knowledge ğŸ—‚ï¸

Use the list of updated facts from `2025_facts.py`.
This data lives **outside the model** and must be retrieved **at query time**.

---

## Step 3 â€” Create a Simple â€œVector Databaseâ€ ğŸ§±

Use one of our Embedders to create a Python dictionary to act as a **mini vector store**.  

âš ï¸ **Note:**  
You are **not building a real vector database**. In production, systems use **vector databases** like FAISS, Pinecone, or Weaviate because:  
- Embeddings are large numeric vectors  
- Systems store millions of documents  
- Fast nearest-neighbor search is required  
- Relational databases cannot efficiently find **semantically closest documents**  

Your in-memory dictionary is just for learning how RAG works.  

---

## Step 4 â€” Implement a RAG Pipeline ğŸšï¸ğŸ”

Write a Python script that:  
1. Takes a user question  
2. Retrieves the `top_k` relevant documents from your mini vector store  
3. Uses an LLM to generate an answer based on the retrieved context  

You can choose any value for `top_k`.  

---

## Step 5 â€” Experiment with `top_k` ğŸ§ª

Try different `top_k` values and reflect:  
- Does `top_k = 1` always work?  
- When does increasing `top_k` improve answers?  
- When does it harm answer quality?  
- How can irrelevant context confuse the model?  
- Summarize the trade-offs of low vs high `top_k`

---

## âœ… Key Takeaway

âœ¨ **RAG doesnâ€™t make an LLM smarter â€” it makes it better informed.**  

The model itself stays the same â€” only the **context and instructions** change.
