# ğŸ§ª Building a Tiny BERT-Style Model (Sentence Embedding Output)

In this exercise, you'll implement a **minimal BERT-style transformer** from scratchâ€”no training required. The focus is on **architecture, tensor shapes**, and producing a **sentence-level embedding** as the model output.

---

## ğŸ¯ Goals

- ğŸ”¹ Understand **bidirectional attention**  
- ğŸ”¹ Implement **transformer encoder blocks manually**  
- ğŸ”¹ Produce **sentence embeddings directly** from the model  
- ğŸ”¹ Integrate **averaging of token vectors** into the model architecture  

---

## ğŸ›  Model Implementation

Create a simple **BERT-style model** with **3 transformer encoder blocks** in PyTorch.

### ğŸ“š Model Configuration

**Tokenizer**  
- Vocabulary size: `20`  

**Embedding**  
- Hidden dimension (`d_model`): `64`  

**Positional Encoding**  
- Maximum sequence length: `32`  

**Attention**  
- Number of heads: `1`  
- Q, K, V dimensions: `d_model Ã— d_model`  
- âš ï¸ **Full attention**: each token attends to all others  

**MLP (Feedforward)**  
- 2 layers with **ReLU** activation  
- Hidden dimension: `128`  

**Sentence Embedding Logic**  
- After the **last transformer block**, the model outputs **token vectors**  
- Compute the **mean of these token vectors along the sequence dimension**  
- The **resulting vector** is the **sentence embedding**  

---

## ğŸŒ€ Forward Pass / Usage

1. ğŸ”¹ Tokenize your input sentence  
2. ğŸ”¹ Pass it through the **embedding + positional encoding**  
3. ğŸ”¹ Pass through the **3 transformer encoder blocks**  
4. ğŸ”¹ Compute the **mean of token vectors** â†’ sentence embedding  
5. ğŸ”¹ Output: single vector of size `d_model`  

---

## ğŸ“ Example

**Input Tokens:**  
`the cat sat on the mat`  

**Model Output:**  
