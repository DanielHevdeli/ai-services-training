# ğŸ§ª Building a Tiny Causal Language Model (Inference-Only)

In this exercise, you'll implement a **minimal GPT-style causal language model from scratch**â€”no training required and **without using PyTorchâ€™s built-in attention or transformer modules**. The focus is on understanding the **architecture**, **tensor shapes**, and performing **inference**.

---

## ğŸ¯ Goals

- ğŸ”¹ Understand how a **causal (autoregressive) language model** works  
- ğŸ”¹ Implement **transformer components manually**  
- ğŸ”¹ Perform **inference on a simple prompt**  
- ğŸ”¹ Build a **next-token generation loop**  
- ğŸ”¹ Implement a **KV-cache** for faster autoregressive generation  

---

## ğŸ›  Model Implementation

Create a simple **Causal LM** with **3 transformer blocks** in PyTorch.  

### ğŸ“š Model Configuration

**Tokenizer**  
- Vocabulary size: `20`  
- Use a dummy vocabulary of your choice  

**Embedding**  
- Hidden dimension (`d_model`): `64`  

**Positional Encoding**  
- Context length: `32`  

**Attention**  
- Number of heads: `1` (simpler to implement)  
- Q, K, V dimensions: `d_model Ã— d_model`  
- âš ï¸ **Causal masking** is important: future tokens must be hidden  

**MLP (Feedforward)**  
- 2 layers with **ReLU** activation  
- Hidden dimension: `128`

Print a summary of the model to see how many parameters each layer has, and how many total parameters are there

---

## ğŸŒ€ Next-Token Generation Loop

To generate text from your model:

1. ğŸ”¹ Start with a **prompt token sequence**  
2. ğŸ”¹ For each **generation step**:
   - Pass the **current sequence** through the model  
   - Take the **logits for the last token**  
   - Apply **softmax** to get probabilities  
   - Pick the **next token** (e.g., max probability)  
   - Append the **new token** to the sequence  
3. ğŸ”¹ Repeat until you reach the **desired sequence length** (e.g., `50` tokens)  

---

## ğŸ’¾ KV-Cache (Optional but Recommended)

To speed up autoregressive generation:

- ğŸ”¹ Store **Keys (K) and Values (V)** for all previous tokens in each transformer block  
- ğŸ”¹ During generation, only compute K and V for the **new token**, then **concatenate** with cached K and V  
- ğŸ”¹ This reduces computation from **O(seqÂ²)** â†’ **O(seq)** per new token  
- ğŸ”¹ Essential for **efficient inference** in long sequences or larger models  

âœ¨ Tip: Try running the same prompt **with and without KV-cache** and compare the timing. You should notice a significant speedup with KV caching!

---
