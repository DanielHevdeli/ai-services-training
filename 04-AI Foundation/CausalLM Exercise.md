# ğŸ§ª Building a Tiny Causal Language Model (Inference-Only)

This exercise guides you through implementing a minimal GPT-style causal language model from scratch, without training, and without using PyTorchâ€™s built-in attention modules.  
The focus is on architecture, tensor shapes, and inference.

## ğŸ¯ Goals

- Understand how a causal (autoregressive) LM works
- Implement transformer components manually
- Perform inference on a simple prompt
- Build a next-token generation loop

# ğŸ§± Part 1 â€” Model Setup

Choose small hyperparameters:

vocab_size   = 20â€“50
max_seq_len  = 16
d_model      = 32
num_layers   = 1
d_hidden     = 64

Create a simple tokenizer: a dictionary mapping words â†’ integers.  
(No BPE, no advanced tokenization.)

# ğŸ”¤ Part 2 â€” Token & Position Embeddings

## 1ï¸âƒ£ Token Embedding

Create a learnable matrix:

token_emb: [vocab_size Ã— d_model]

Lookup each token and stack them.

## 2ï¸âƒ£ Positional Embedding

Create:

pos_emb: [max_seq_len Ã— d_model]

Add embeddings element-wise:

X = token_emb[tokens] + pos_emb[positions]

Shape: (seq_len, d_model)

# ğŸ§  Part 3 â€” Self-Attention (Manual Implementation)

Implement single-head attention manually.

### Linear projections

Q = X @ Wq      # [seq_len Ã— d_model]  
K = X @ Wk  
V = X @ Wv  

### Scaled dot-product attention

scores = Q @ K.T / sqrt(d_model)

### Causal mask (autoregressive)

Apply a lower-triangular mask so token t attends only to tokens â‰¤ t:

mask[i, j] = 0 if j <= i else -inf

scores += mask  
attn = softmax(scores)

### Attention output

out = attn @ V

# âš™ï¸ Part 4 â€” Feed-Forward (MLP) Block

Two linear layers + activation:

X â†’ Linear(d_model â†’ d_hidden)  
  â†’ GELU / ReLU  
  â†’ Linear(d_hidden â†’ d_model)

# ğŸ§© Part 5 â€” Transformer Block

Use residual connections:

X = X + Attention(LayerNorm(X))  
X = X + MLP(LayerNorm(X))

(LayerNorm is optional for simplicity.)

# ğŸ”š Part 6 â€” LM Head (Projection to Vocabulary)

Final linear projection:

W_out: [d_model Ã— vocab_size]  
b_out: [vocab_size]

Output logits:

logits = X @ W_out + b_out

Output shape: (seq_len, vocab_size)

For next-token prediction, use only the last row:

next_logits = logits[-1]

# ğŸš€ Part 7 â€” Inference Flow

1. Tokenize the input prompt  
2. Pass tokens through:
   - Embeddings  
   - Transformer block  
   - LM head  
3. Take:

logits[-1] â†’ softmax â†’ choose next token

# ğŸ” Part 8 â€” Generation Loop

for step in range(max_new_tokens):  
    logits = model(current_tokens)  
    probs = softmax(logits[-1])  
    next_token = argmax or sample(probs)  
    append(next_token)

Stop on: EOS token or max token count

# ğŸ‰ You're Done!

You have implemented a tiny causal language model for **inference only**, covering embeddings, attention, causal masking, transformer blocks, and next-token generation.
